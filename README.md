# ğŸš€ Local LLM Test Case Generator

A privacy-first, AI-powered tool that generates detailed software test cases from natural language descriptions. Built with **Ollama** (Llama 3.2), **FastAPI**, and a premium **Vanilla JS Frontend**.

![Status](https://img.shields.io/badge/Status-Active-success)
![LLM](https://img.shields.io/badge/LLM-Llama3.2-blue)
![Architecture](https://img.shields.io/badge/Architecture-3--Layer%20BLAST-purple)

## âœ¨ Features

-   **ğŸ”’ Privacy First**: Runs 100% locally using Ollama. No API keys, no data leaves your machine.
-   **ğŸ›¡ï¸ Anti-Hallucination Rules**: Integrated strict verification logic to ensure test cases are grounded in facts.
-   **ğŸ§  Self-Healing Logic**: Uses robust JSON parsing and Pydantic validation to ensure the LLM returns structured data every time.
-   **ğŸ¨ Premium UI**: A clean, "Glassmorphism" inspired dark mode interface with real-time verification status.
-   **âš¡ Fast & Lightweight**: Backend powered by FastAPI, Frontend is pure HTML/CSS/JS (no heavy npm build steps).

## ğŸ—ï¸ Architecture

This project follows the **BLAST Protocol** and **A.N.T. 3-Layer Architecture**:

1.  **Layer 1 (Architecture)**: Defined SOPS in `architecture/` to govern logic.
2.  **Layer 2 (Navigation)**: `backend/server.py` routes data and handles API requests.
3.  **Layer 3 (Tools)**: `backend/services/llm_service.py` executes the deterministic logic.

## ğŸš€ Getting Started

### Prerequisites

-   **Python 3.8+**
-   **[Ollama](https://ollama.com/)** installed and running.
-   Pull the required model:
    ```bash
    ollama pull llama3.2
    ```

### Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/yogeshcodeshare/AI_Project1-LocatTestCaseGenratorAgent.git
    cd AI_Project1-LocatTestCaseGenratorAgent
    ```

2.  **Set up Virtual Environment**:
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

## ğŸƒâ€â™‚ï¸ Usage

### Quick Start (Recommended)
You can start both the backend and frontend with a single command:
```bash
./start.sh
```

### Manual Start
#### 1. Start the Backend
The backend runs on port `8000`.
```bash
./venv/bin/uvicorn backend.server:app --host 0.0.0.0 --port 8000 --reload
```

#### 2. Start the Frontend
Serve the frontend with any static file server:
```bash
cd frontend
python3 -m http.server 8080
```

### 3. Generate Test Cases
-   Open your browser to `http://localhost:8080`.
-   Type a feature description.
-   Click **Generate**.
-   The AI will now show **Verified Facts** and **Missing Information** alongside your test cases!

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ architecture/        # Technical SOPs and Logic definitions
â”œâ”€â”€ backend/             # FastAPI Server & LLM Logic
â”‚   â”œâ”€â”€ models.py        # Pydantic Schemas
â”‚   â”œâ”€â”€ server.py        # API Entrypoint
â”‚   â””â”€â”€ services/        # Business Logic (Ollama integration)
â”œâ”€â”€ frontend/            # Static Web UI (HTML/CSS/JS)
â”œâ”€â”€ tools/               # Utility scripts (e.g., verification)
â”œâ”€â”€ BLAST.md             # Master System Prompt & Progress Log
â””â”€â”€ requirements.txt     # Python dependencies
```

## ğŸ¤ Contributing

This project is a demonstration of **Agentic Coding** with the BLAST protocol. Feel free to fork and enhance!

---
*Generated by Antigravity Agent*
